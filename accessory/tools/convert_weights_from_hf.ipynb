{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/.local/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.12s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(\"/home/\", \"huggingface\")\n",
    "os.environ[\"HF_HUB_CACHE\"] = os.path.join(\"/home/\", \"huggingface\")\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "params = {\n",
    "    \"dim\": 4096,\n",
    "    \"n_layers\": 32,\n",
    "    \"head_dim\": 128,\n",
    "    \"hidden_dim\": 14336,\n",
    "    \"n_heads\": 32,\n",
    "    \"n_kv_heads\": 8,\n",
    "    \"norm_eps\": 1e-05,\n",
    "    \"sliding_window\": 4096,\n",
    "    \"vocab_size\": 32000\n",
    "}\n",
    "num_shards = 1\n",
    "n_layers = params[\"n_layers\"]\n",
    "n_heads = params[\"n_heads\"]\n",
    "n_heads_per_shard = n_heads // num_shards\n",
    "dim = params[\"dim\"]\n",
    "dims_per_head = dim // n_heads\n",
    "base = params.get(\"rope_theta\", 100000.0)\n",
    "inv_freq = 1.0 / (base ** (torch.arange(0, dims_per_head, 2).float() / dims_per_head))\n",
    "max_position_embeddings = 4096 * 8\n",
    "\n",
    "if \"n_kv_heads\" in params:\n",
    "    num_key_value_heads = params[\"n_kv_heads\"]  # for GQA / MQA\n",
    "    num_local_key_value_heads = num_key_value_heads // num_shards\n",
    "    key_value_dim = dims_per_head * num_local_key_value_heads\n",
    "else: \n",
    "    num_key_value_heads = n_heads\n",
    "    num_local_key_value_heads = n_heads_per_shard\n",
    "    key_value_dim = dim\n",
    "\n",
    "def permute(w, n_heads=n_heads, dim1, dim2):\n",
    "    return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n",
    "\n",
    "def reverse_permute(w, n_heads, dim1, dim2):\n",
    "    return w.view(n_heads, 2, dim1 // n_heads // 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = {}\n",
    "\n",
    "replacements = {\n",
    "    \".embed_tokens.\": \".tok_embeddings.\",\n",
    "    \".self_attn.\": \".attention.\",\n",
    "    \".q_proj.\": \".wq.\",\n",
    "    \".k_proj.\": \".wk.\",\n",
    "    \".v_proj.\": \".wv.\",\n",
    "    \".o_proj.\": \".wo.\",\n",
    "    \".mlp.\": \".feed_forward.\",\n",
    "    \".gate_proj.\": \".w1.\",\n",
    "    \".up_proj.\": \".w3.\",\n",
    "    \".down_proj.\": \".w2.\",\n",
    "    \".input_layernorm.\": \".attention_norm.\",\n",
    "    \".post_attention_layernorm.\": \".ffn_norm.\",\n",
    "    \".lm_head.\": \".output.\",\n",
    "    \".norm.\": \".norm.\"\n",
    "}\n",
    "\n",
    "for key, val in model.state_dict().items():\n",
    "    # change shape for some weights\n",
    "    if \"k_proj\" in key:\n",
    "        val = reverse_permute(\n",
    "            val,\n",
    "            num_key_value_heads,\n",
    "            key_value_dim,\n",
    "            dim,\n",
    "        )\n",
    "    elif \"q_proj\" in key:\n",
    "        val = reverse_permute(\n",
    "            val,\n",
    "            n_heads,\n",
    "            dim,\n",
    "            dim,\n",
    "        )\n",
    "    key = key.replace(\"model.\", \"\")\n",
    "    key = key.replace(\"blocks.\", \"layers.\")\n",
    "    key = \"llma.\" + key\n",
    "    # replace\n",
    "    for old, new in replacements.items():\n",
    "        key = key.replace(old, new)\n",
    "\n",
    "    new_state_dict[key] = val\n",
    "\n",
    "new_state_dict = {\"model\": new_state_dict}\n",
    "\n",
    "torch.save(new_state_dict, \"consolidated.00-of-01.model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the model is saved correctly\n",
    "model_original = torch.load(\"/home/mistral-src-main/mistral-7B-v0.1/consolidated.00.pth\")\n",
    "model_converted = torch.load(\"consolidated.00-of-01.model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeddings.weight is the same\n",
      "norm.weight is the same\n",
      "output.weight is the same\n",
      "layers.0.attention.wq.weight is the same\n",
      "layers.0.attention.wk.weight is the same\n",
      "layers.0.attention.wv.weight is the same\n",
      "layers.0.attention.wo.weight is the same\n",
      "layers.0.feed_forward.w1.weight is the same\n",
      "layers.0.feed_forward.w2.weight is the same\n",
      "layers.0.feed_forward.w3.weight is the same\n",
      "layers.0.attention_norm.weight is the same\n",
      "layers.0.ffn_norm.weight is the same\n",
      "layers.1.attention.wq.weight is the same\n",
      "layers.1.attention.wk.weight is the same\n",
      "layers.1.attention.wv.weight is the same\n",
      "layers.1.attention.wo.weight is the same\n",
      "layers.1.feed_forward.w1.weight is the same\n",
      "layers.1.feed_forward.w2.weight is the same\n",
      "layers.1.feed_forward.w3.weight is the same\n",
      "layers.1.attention_norm.weight is the same\n",
      "layers.1.ffn_norm.weight is the same\n",
      "layers.2.attention.wq.weight is the same\n",
      "layers.2.attention.wk.weight is the same\n",
      "layers.2.attention.wv.weight is the same\n",
      "layers.2.attention.wo.weight is the same\n",
      "layers.2.feed_forward.w1.weight is the same\n",
      "layers.2.feed_forward.w2.weight is the same\n",
      "layers.2.feed_forward.w3.weight is the same\n",
      "layers.2.attention_norm.weight is the same\n",
      "layers.2.ffn_norm.weight is the same\n",
      "layers.3.attention.wq.weight is the same\n",
      "layers.3.attention.wk.weight is the same\n",
      "layers.3.attention.wv.weight is the same\n",
      "layers.3.attention.wo.weight is the same\n",
      "layers.3.feed_forward.w1.weight is the same\n",
      "layers.3.feed_forward.w2.weight is the same\n",
      "layers.3.feed_forward.w3.weight is the same\n",
      "layers.3.attention_norm.weight is the same\n",
      "layers.3.ffn_norm.weight is the same\n",
      "layers.4.attention.wq.weight is the same\n",
      "layers.4.attention.wk.weight is the same\n",
      "layers.4.attention.wv.weight is the same\n",
      "layers.4.attention.wo.weight is the same\n",
      "layers.4.feed_forward.w1.weight is the same\n",
      "layers.4.feed_forward.w2.weight is the same\n",
      "layers.4.feed_forward.w3.weight is the same\n",
      "layers.4.attention_norm.weight is the same\n",
      "layers.4.ffn_norm.weight is the same\n",
      "layers.5.attention.wq.weight is the same\n",
      "layers.5.attention.wk.weight is the same\n",
      "layers.5.attention.wv.weight is the same\n",
      "layers.5.attention.wo.weight is the same\n",
      "layers.5.feed_forward.w1.weight is the same\n",
      "layers.5.feed_forward.w2.weight is the same\n",
      "layers.5.feed_forward.w3.weight is the same\n",
      "layers.5.attention_norm.weight is the same\n",
      "layers.5.ffn_norm.weight is the same\n",
      "layers.6.attention.wq.weight is the same\n",
      "layers.6.attention.wk.weight is the same\n",
      "layers.6.attention.wv.weight is the same\n",
      "layers.6.attention.wo.weight is the same\n",
      "layers.6.feed_forward.w1.weight is the same\n",
      "layers.6.feed_forward.w2.weight is the same\n",
      "layers.6.feed_forward.w3.weight is the same\n",
      "layers.6.attention_norm.weight is the same\n",
      "layers.6.ffn_norm.weight is the same\n",
      "layers.7.attention.wq.weight is the same\n",
      "layers.7.attention.wk.weight is the same\n",
      "layers.7.attention.wv.weight is the same\n",
      "layers.7.attention.wo.weight is the same\n",
      "layers.7.feed_forward.w1.weight is the same\n",
      "layers.7.feed_forward.w2.weight is the same\n",
      "layers.7.feed_forward.w3.weight is the same\n",
      "layers.7.attention_norm.weight is the same\n",
      "layers.7.ffn_norm.weight is the same\n",
      "layers.8.attention.wq.weight is the same\n",
      "layers.8.attention.wk.weight is the same\n",
      "layers.8.attention.wv.weight is the same\n",
      "layers.8.attention.wo.weight is the same\n",
      "layers.8.feed_forward.w1.weight is the same\n",
      "layers.8.feed_forward.w2.weight is the same\n",
      "layers.8.feed_forward.w3.weight is the same\n",
      "layers.8.attention_norm.weight is the same\n",
      "layers.8.ffn_norm.weight is the same\n",
      "layers.9.attention.wq.weight is the same\n",
      "layers.9.attention.wk.weight is the same\n",
      "layers.9.attention.wv.weight is the same\n",
      "layers.9.attention.wo.weight is the same\n",
      "layers.9.feed_forward.w1.weight is the same\n",
      "layers.9.feed_forward.w2.weight is the same\n",
      "layers.9.feed_forward.w3.weight is the same\n",
      "layers.9.attention_norm.weight is the same\n",
      "layers.9.ffn_norm.weight is the same\n",
      "layers.10.attention.wq.weight is the same\n",
      "layers.10.attention.wk.weight is the same\n",
      "layers.10.attention.wv.weight is the same\n",
      "layers.10.attention.wo.weight is the same\n",
      "layers.10.feed_forward.w1.weight is the same\n",
      "layers.10.feed_forward.w2.weight is the same\n",
      "layers.10.feed_forward.w3.weight is the same\n",
      "layers.10.attention_norm.weight is the same\n",
      "layers.10.ffn_norm.weight is the same\n",
      "layers.11.attention.wq.weight is the same\n",
      "layers.11.attention.wk.weight is the same\n",
      "layers.11.attention.wv.weight is the same\n",
      "layers.11.attention.wo.weight is the same\n",
      "layers.11.feed_forward.w1.weight is the same\n",
      "layers.11.feed_forward.w2.weight is the same\n",
      "layers.11.feed_forward.w3.weight is the same\n",
      "layers.11.attention_norm.weight is the same\n",
      "layers.11.ffn_norm.weight is the same\n",
      "layers.12.attention.wq.weight is the same\n",
      "layers.12.attention.wk.weight is the same\n",
      "layers.12.attention.wv.weight is the same\n",
      "layers.12.attention.wo.weight is the same\n",
      "layers.12.feed_forward.w1.weight is the same\n",
      "layers.12.feed_forward.w2.weight is the same\n",
      "layers.12.feed_forward.w3.weight is the same\n",
      "layers.12.attention_norm.weight is the same\n",
      "layers.12.ffn_norm.weight is the same\n",
      "layers.13.attention.wq.weight is the same\n",
      "layers.13.attention.wk.weight is the same\n",
      "layers.13.attention.wv.weight is the same\n",
      "layers.13.attention.wo.weight is the same\n",
      "layers.13.feed_forward.w1.weight is the same\n",
      "layers.13.feed_forward.w2.weight is the same\n",
      "layers.13.feed_forward.w3.weight is the same\n",
      "layers.13.attention_norm.weight is the same\n",
      "layers.13.ffn_norm.weight is the same\n",
      "layers.14.attention.wq.weight is the same\n",
      "layers.14.attention.wk.weight is the same\n",
      "layers.14.attention.wv.weight is the same\n",
      "layers.14.attention.wo.weight is the same\n",
      "layers.14.feed_forward.w1.weight is the same\n",
      "layers.14.feed_forward.w2.weight is the same\n",
      "layers.14.feed_forward.w3.weight is the same\n",
      "layers.14.attention_norm.weight is the same\n",
      "layers.14.ffn_norm.weight is the same\n",
      "layers.15.attention.wq.weight is the same\n",
      "layers.15.attention.wk.weight is the same\n",
      "layers.15.attention.wv.weight is the same\n",
      "layers.15.attention.wo.weight is the same\n",
      "layers.15.feed_forward.w1.weight is the same\n",
      "layers.15.feed_forward.w2.weight is the same\n",
      "layers.15.feed_forward.w3.weight is the same\n",
      "layers.15.attention_norm.weight is the same\n",
      "layers.15.ffn_norm.weight is the same\n",
      "layers.16.attention.wq.weight is the same\n",
      "layers.16.attention.wk.weight is the same\n",
      "layers.16.attention.wv.weight is the same\n",
      "layers.16.attention.wo.weight is the same\n",
      "layers.16.feed_forward.w1.weight is the same\n",
      "layers.16.feed_forward.w2.weight is the same\n",
      "layers.16.feed_forward.w3.weight is the same\n",
      "layers.16.attention_norm.weight is the same\n",
      "layers.16.ffn_norm.weight is the same\n",
      "layers.17.attention.wq.weight is the same\n",
      "layers.17.attention.wk.weight is the same\n",
      "layers.17.attention.wv.weight is the same\n",
      "layers.17.attention.wo.weight is the same\n",
      "layers.17.feed_forward.w1.weight is the same\n",
      "layers.17.feed_forward.w2.weight is the same\n",
      "layers.17.feed_forward.w3.weight is the same\n",
      "layers.17.attention_norm.weight is the same\n",
      "layers.17.ffn_norm.weight is the same\n",
      "layers.18.attention.wq.weight is the same\n",
      "layers.18.attention.wk.weight is the same\n",
      "layers.18.attention.wv.weight is the same\n",
      "layers.18.attention.wo.weight is the same\n",
      "layers.18.feed_forward.w1.weight is the same\n",
      "layers.18.feed_forward.w2.weight is the same\n",
      "layers.18.feed_forward.w3.weight is the same\n",
      "layers.18.attention_norm.weight is the same\n",
      "layers.18.ffn_norm.weight is the same\n",
      "layers.19.attention.wq.weight is the same\n",
      "layers.19.attention.wk.weight is the same\n",
      "layers.19.attention.wv.weight is the same\n",
      "layers.19.attention.wo.weight is the same\n",
      "layers.19.feed_forward.w1.weight is the same\n",
      "layers.19.feed_forward.w2.weight is the same\n",
      "layers.19.feed_forward.w3.weight is the same\n",
      "layers.19.attention_norm.weight is the same\n",
      "layers.19.ffn_norm.weight is the same\n",
      "layers.20.attention.wq.weight is the same\n",
      "layers.20.attention.wk.weight is the same\n",
      "layers.20.attention.wv.weight is the same\n",
      "layers.20.attention.wo.weight is the same\n",
      "layers.20.feed_forward.w1.weight is the same\n",
      "layers.20.feed_forward.w2.weight is the same\n",
      "layers.20.feed_forward.w3.weight is the same\n",
      "layers.20.attention_norm.weight is the same\n",
      "layers.20.ffn_norm.weight is the same\n",
      "layers.21.attention.wq.weight is the same\n",
      "layers.21.attention.wk.weight is the same\n",
      "layers.21.attention.wv.weight is the same\n",
      "layers.21.attention.wo.weight is the same\n",
      "layers.21.feed_forward.w1.weight is the same\n",
      "layers.21.feed_forward.w2.weight is the same\n",
      "layers.21.feed_forward.w3.weight is the same\n",
      "layers.21.attention_norm.weight is the same\n",
      "layers.21.ffn_norm.weight is the same\n",
      "layers.22.attention.wq.weight is the same\n",
      "layers.22.attention.wk.weight is the same\n",
      "layers.22.attention.wv.weight is the same\n",
      "layers.22.attention.wo.weight is the same\n",
      "layers.22.feed_forward.w1.weight is the same\n",
      "layers.22.feed_forward.w2.weight is the same\n",
      "layers.22.feed_forward.w3.weight is the same\n",
      "layers.22.attention_norm.weight is the same\n",
      "layers.22.ffn_norm.weight is the same\n",
      "layers.23.attention.wq.weight is the same\n",
      "layers.23.attention.wk.weight is the same\n",
      "layers.23.attention.wv.weight is the same\n",
      "layers.23.attention.wo.weight is the same\n",
      "layers.23.feed_forward.w1.weight is the same\n",
      "layers.23.feed_forward.w2.weight is the same\n",
      "layers.23.feed_forward.w3.weight is the same\n",
      "layers.23.attention_norm.weight is the same\n",
      "layers.23.ffn_norm.weight is the same\n",
      "layers.24.attention.wq.weight is the same\n",
      "layers.24.attention.wk.weight is the same\n",
      "layers.24.attention.wv.weight is the same\n",
      "layers.24.attention.wo.weight is the same\n",
      "layers.24.feed_forward.w1.weight is the same\n",
      "layers.24.feed_forward.w2.weight is the same\n",
      "layers.24.feed_forward.w3.weight is the same\n",
      "layers.24.attention_norm.weight is the same\n",
      "layers.24.ffn_norm.weight is the same\n",
      "layers.25.attention.wq.weight is the same\n",
      "layers.25.attention.wk.weight is the same\n",
      "layers.25.attention.wv.weight is the same\n",
      "layers.25.attention.wo.weight is the same\n",
      "layers.25.feed_forward.w1.weight is the same\n",
      "layers.25.feed_forward.w2.weight is the same\n",
      "layers.25.feed_forward.w3.weight is the same\n",
      "layers.25.attention_norm.weight is the same\n",
      "layers.25.ffn_norm.weight is the same\n",
      "layers.26.attention.wq.weight is the same\n",
      "layers.26.attention.wk.weight is the same\n",
      "layers.26.attention.wv.weight is the same\n",
      "layers.26.attention.wo.weight is the same\n",
      "layers.26.feed_forward.w1.weight is the same\n",
      "layers.26.feed_forward.w2.weight is the same\n",
      "layers.26.feed_forward.w3.weight is the same\n",
      "layers.26.attention_norm.weight is the same\n",
      "layers.26.ffn_norm.weight is the same\n",
      "layers.27.attention.wq.weight is the same\n",
      "layers.27.attention.wk.weight is the same\n",
      "layers.27.attention.wv.weight is the same\n",
      "layers.27.attention.wo.weight is the same\n",
      "layers.27.feed_forward.w1.weight is the same\n",
      "layers.27.feed_forward.w2.weight is the same\n",
      "layers.27.feed_forward.w3.weight is the same\n",
      "layers.27.attention_norm.weight is the same\n",
      "layers.27.ffn_norm.weight is the same\n",
      "layers.28.attention.wq.weight is the same\n",
      "layers.28.attention.wk.weight is the same\n",
      "layers.28.attention.wv.weight is the same\n",
      "layers.28.attention.wo.weight is the same\n",
      "layers.28.feed_forward.w1.weight is the same\n",
      "layers.28.feed_forward.w2.weight is the same\n",
      "layers.28.feed_forward.w3.weight is the same\n",
      "layers.28.attention_norm.weight is the same\n",
      "layers.28.ffn_norm.weight is the same\n",
      "layers.29.attention.wq.weight is the same\n",
      "layers.29.attention.wk.weight is the same\n",
      "layers.29.attention.wv.weight is the same\n",
      "layers.29.attention.wo.weight is the same\n",
      "layers.29.feed_forward.w1.weight is the same\n",
      "layers.29.feed_forward.w2.weight is the same\n",
      "layers.29.feed_forward.w3.weight is the same\n",
      "layers.29.attention_norm.weight is the same\n",
      "layers.29.ffn_norm.weight is the same\n",
      "layers.30.attention.wq.weight is the same\n",
      "layers.30.attention.wk.weight is the same\n",
      "layers.30.attention.wv.weight is the same\n",
      "layers.30.attention.wo.weight is the same\n",
      "layers.30.feed_forward.w1.weight is the same\n",
      "layers.30.feed_forward.w2.weight is the same\n",
      "layers.30.feed_forward.w3.weight is the same\n",
      "layers.30.attention_norm.weight is the same\n",
      "layers.30.ffn_norm.weight is the same\n",
      "layers.31.attention.wq.weight is the same\n",
      "layers.31.attention.wk.weight is the same\n",
      "layers.31.attention.wv.weight is the same\n",
      "layers.31.attention.wo.weight is the same\n",
      "layers.31.feed_forward.w1.weight is the same\n",
      "layers.31.feed_forward.w2.weight is the same\n",
      "layers.31.feed_forward.w3.weight is the same\n",
      "layers.31.attention_norm.weight is the same\n",
      "layers.31.ffn_norm.weight is the same\n"
     ]
    }
   ],
   "source": [
    "# check model weights are the same\n",
    "for k, v in model_original.items():\n",
    "    if torch.allclose(v, model_converted['model']['llma.' + k].to(dtype=torch.bfloat16)):\n",
    "        print(f\"{k} is the same\")\n",
    "    else:\n",
    "        print(f\"********************{k} is different********************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
